{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "id": "74a5129c",
   "cell_type": "markdown",
   "source": "# MotoGP Lap Time Prediction - Model Improvement\n\nThis notebook focuses on improving the model performance through feature importance analysis and hyperparameter tuning.",
   "metadata": {}
  },
  {
   "id": "c9876cda",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.feature_selection import SelectFromModel\nimport warnings\nwarnings.filterwarnings('ignore')",
   "outputs": []
  },
  {
   "id": "bcc8c8bc",
   "cell_type": "markdown",
   "source": "## Load and Prepare Data",
   "metadata": {}
  },
  {
   "id": "10182813",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "1. Load Data\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\nprint('Train shape:', train_data.shape)\nprint('Test shape:', test_data.shape)",
   "outputs": []
  },
  {
   "id": "ea56a844",
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {}
  },
  {
   "id": "45450bd2",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "2. Outlier Handling (IQR Method)\nQ1 = train_data['Lap_Time_Seconds'].quantile(0.25)\nQ3 = train_data['Lap_Time_Seconds'].quantile(0.75)\nIQR = Q3 - Q1\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\nfiltered_train = train_data[(train_data['Lap_Time_Seconds'] >= lower) & (train_data['Lap_Time_Seconds'] <= upper)]\nprint(f'Original train size: {len(train_data)}, After outlier removal: {len(filtered_train)}')",
   "outputs": []
  },
  {
   "id": "8a64094d",
   "cell_type": "markdown",
   "source": "## Feature Importance Analysis",
   "metadata": {}
  },
  {
   "id": "2e15d311",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "3. Preprocessing and Feature Selection\ncategorical_cols = filtered_train.select_dtypes(include=['object']).columns\nnumerical_cols = filtered_train.select_dtypes(include=[np.number]).columns.drop('Lap_Time_Seconds')\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), numerical_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n])\nX = filtered_train.drop('Lap_Time_Seconds', axis=1)\ny = filtered_train['Lap_Time_Seconds']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_prep = preprocessor.fit_transform(X_train)\nX_val_prep = preprocessor.transform(X_val)\ntest_prep = preprocessor.transform(test_data)",
   "outputs": []
  },
  {
   "id": "56ccba46",
   "cell_type": "markdown",
   "source": "## Model Training and Hyperparameter Tuning",
   "metadata": {}
  },
  {
   "id": "b237855e",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "Feature selection using XGBoost\nxgb_selector = XGBRegressor(n_estimators=100, random_state=42)\nxgb_selector.fit(X_train_prep, y_train)\nselector = SelectFromModel(xgb_selector, prefit=True, threshold='median')\nX_train_sel = selector.transform(X_train_prep)\nX_val_sel = selector.transform(X_val_prep)\ntest_sel = selector.transform(test_prep)\nprint('Selected features:', X_train_sel.shape[1])",
   "outputs": []
  },
  {
   "id": "c8b8cfe0",
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "metadata": {}
  },
  {
   "id": "97c830d5",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "4. Model Training: XGBoost and LightGBM\nxgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=7, subsample=0.9, colsample_bytree=0.9, random_state=42)\nlgbm = LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=7, subsample=0.9, colsample_bytree=0.9, random_state=42)\nxgb.fit(X_train_sel, y_train)\nlgbm.fit(X_train_sel, y_train)\nxgb_pred = xgb.predict(X_val_sel)\nlgbm_pred = lgbm.predict(X_val_sel)\nprint('XGBoost RMSE:', np.sqrt(mean_squared_error(y_val, xgb_pred)))\nprint('LightGBM RMSE:', np.sqrt(mean_squared_error(y_val, lgbm_pred)))",
   "outputs": []
  },
  {
   "id": "1ceab1e6",
   "cell_type": "markdown",
   "source": "## Generate Predictions for Test Data",
   "metadata": {}
  },
  {
   "id": "036cffda",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "5. Ensemble (Averaging)\nensemble_pred = (xgb_pred + lgbm_pred) / 2\nensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\nprint('Ensemble RMSE:', ensemble_rmse)",
   "outputs": []
  }
 ]
}